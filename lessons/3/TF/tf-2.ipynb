{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0-dev20191215'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "c = a + b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Больше никаких сессий\n",
    "\n",
    "* В tf1.x вы вручную создавали граф, а затем компилировали его и обсчитывали результаты при помощи `sess.run`\n",
    "* В tf2 сессия - деталь реализации, и значения вычисляются как в \"обычном\" python\n",
    "* `sess.run` заменен на функции. Подробности далее\n",
    "* Не нужны `tf.global_variables_initializer()` и прочие глобальные переменные (хотя мы их не разбирали)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полносвязная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x7fa1385aba58>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(x, W, b, activation):\n",
    "    # return activation(x @ W + b)\n",
    "    return activation(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(1, 10)\n",
    "W = np.random.randn(10, 10)\n",
    "b = np.random.randn(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.47382476, -0.13733021, -1.8861317 ,  0.47889112,  2.17733765,\n",
       "        -3.17548282,  5.72190234, -1.55847526, -0.59147654,  3.38809699]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.98872496 0.4657213  0.13168616 0.617486   0.89819588 0.04009884\n",
      "  0.9967372  0.17386555 0.35629614 0.96733046]], shape=(1, 10), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[4.47382476 0.         0.         0.47889112 2.17733765 0.\n",
      "  5.72190234 0.         0.         3.38809699]], shape=(1, 10), dtype=float64)\n",
      "[[ 4.47382476 -0.13733021 -1.8861317   0.47889112  2.17733765 -3.17548282\n",
      "   5.72190234 -1.55847526 -0.59147654  3.38809699]]\n"
     ]
    }
   ],
   "source": [
    "print(dense(x, W, b, tf.sigmoid))\n",
    "print(dense(x, W, b, tf.nn.relu))\n",
    "print(dense(x, W, b, lambda x: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.47382476, 0.        , 0.        , 0.47889112, 2.17733765,\n",
       "        0.        , 5.72190234, 0.        , 0.        , 3.38809699]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = dense(x, W, b, tf.nn.relu)\n",
    "type(t.numpy())\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Документация: https://keras.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_tr, y_tr), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTSequence(keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size, preprocess: Callable = None):\n",
    "        super().__init__()\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._preprocess_fn = preprocess\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "    def _preprocess(self, X, y):\n",
    "        if self._preprocess_fn is not None:\n",
    "            return self._preprocess_fn(X, y)\n",
    "        return (X / 255.).reshape((-1, 28*28)), y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self._X) / float(self._batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = slice(idx*self._batch_size, (idx+1)*self._batch_size, 1)\n",
    "        x_batch = self._X[batch_idx]\n",
    "        y_batch = self._y[batch_idx]\n",
    "        return self._preprocess(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = MNISTSequence(X_tr, y_tr, 128)\n",
    "test_seq = MNISTSequence(X_test, y_test, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_seq)\n",
    "x_batch, y_batch = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_seq:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Декоратор tf.function\n",
    "\n",
    "> Трансформирует функцию на python, в том числе циклы, if-else и т.д., в код tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**инициализация** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(784, 128) dtype=float64, numpy=\n",
       "array([[0.89680368, 0.37815468, 0.20279992, ..., 0.97394806, 0.15416133,\n",
       "        0.84316236],\n",
       "       [0.08898942, 0.73674456, 0.23349083, ..., 0.93695346, 0.79359374,\n",
       "        0.22899672],\n",
       "       [0.29841466, 0.06071232, 0.73178058, ..., 0.74884755, 0.66460804,\n",
       "        0.09951496],\n",
       "       ...,\n",
       "       [0.00322459, 0.6392351 , 0.77039475, ..., 0.25717034, 0.31391178,\n",
       "        0.88196372],\n",
       "       [0.11258042, 0.27393739, 0.00170766, ..., 0.16015985, 0.76814886,\n",
       "        0.2112376 ],\n",
       "       [0.11743671, 0.95546248, 0.94452555, ..., 0.80189773, 0.35165856,\n",
       "        0.18717889]])>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Variable(np.random.rand(784, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(np.random.randn(784, 128))\n",
    "b1 = tf.Variable(np.zeros((1, 128)))\n",
    "W2 = tf.Variable(np.random.randn(128, 10))\n",
    "b2 = tf.Variable(np.zeros((1, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"модель\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def mlp(x, W1, b1, a1, W2, b2, a2):\n",
    "    x = dense(x, W1, b1, a1)\n",
    "    x = dense(x, W2, b2, a2)\n",
    "    return x\n",
    "\n",
    "\n",
    "model = partial(mlp, W1=W1, b1=b1, a1=tf.nn.sigmoid, W2=W2, b2=b2, a2=tf.nn.sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Как вычислить значения меток?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03125"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_batch, y_test_batch = next(iter(test_seq))\n",
    "y_test_predicted = np.argmax(model(x_test_batch).numpy(), axis=1)\n",
    "accuracy_on_batch = sum(y_test_batch == y_test_predicted) / len(y_test_batch)\n",
    "accuracy_on_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Все необходимое для оптимизации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "trainable_variables = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Цикл обучения: одна эпоха**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_seq:\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(x)\n",
    "        loss = keras.losses.sparse_categorical_crossentropy(y, prediction)\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Волшебство!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84375"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted = np.argmax(model(x_test_batch).numpy(), axis=1)\n",
    "accuracy_on_batch = sum(y_test_batch == y_test_predicted) / len(y_test_batch)\n",
    "accuracy_on_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полноценная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(784, 100) dtype=float64, numpy=\n",
       "array([[0.07354145, 0.95816226, 0.86958152, ..., 0.29124977, 0.4189878 ,\n",
       "        0.92840866],\n",
       "       [0.99050818, 0.99001549, 0.38769032, ..., 0.21420077, 0.005782  ,\n",
       "        0.62489352],\n",
       "       [0.39288026, 0.07498111, 0.06225706, ..., 0.53861557, 0.33892961,\n",
       "        0.84317689],\n",
       "       ...,\n",
       "       [0.82507586, 0.31540101, 0.01527163, ..., 0.79641558, 0.77753827,\n",
       "        0.60920997],\n",
       "       [0.51993505, 0.38027543, 0.43004574, ..., 0.3533058 , 0.52283467,\n",
       "        0.63536416],\n",
       "       [0.06491503, 0.71965324, 0.09744576, ..., 0.21763117, 0.50965099,\n",
       "        0.08717073]])>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Variable(np.random.rand(784, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, inp_shape, out_shape, activation: Callable):\n",
    "        self.trainable = True\n",
    "        self._inp_shape = inp_shape\n",
    "        self._out_shape = out_shape\n",
    "        self._activation = activation\n",
    "        if 'sigmoid' in self._activation.__name__:\n",
    "            self._w = tf.Variable(np.random.rand(inp_shape, out_shape) * np.sqrt(6 / (inp_shape + out_shape)))\n",
    "        elif 'relu' in self._activation.__name__:\n",
    "            self._w = tf.Variable(np.random.randn(inp_shape, out_shape) * np.sqrt(2 / (inp_shape)))\n",
    "        else:\n",
    "            # Just a Normal\n",
    "            self._w = tf.Variable(np.random.randn(inp_shape, out_shape))\n",
    "        self._b = tf.Variable(np.zeros((1, out_shape)))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self._activation(x @ self._w + self._b)\n",
    "    \n",
    "    def get_trainable(self):\n",
    "        if self.trainable: \n",
    "            return [self._w, self._b]\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    @property\n",
    "    def inp_shape(self):\n",
    "        return self._inp_shape\n",
    "    \n",
    "    @property\n",
    "    def out_shape(self):\n",
    "        return self._out_shape\n",
    "    \n",
    "    @property\n",
    "    def w(self):\n",
    "        return self._w\n",
    "    \n",
    "    @property\n",
    "    def b(self):\n",
    "        return self._b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, *args):\n",
    "        self._layers = args\n",
    "        self._trainable_variables = [i for s in [l.get_trainable() for l in self._layers] for i in s] \n",
    "        \n",
    "    @tf.function\n",
    "    def _forward(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "        \n",
    "    def fit_generator(self, train_seq, eval_seq, epoch, loss, optimizer):\n",
    "        history = dict(train=list(), val=list())\n",
    "        \n",
    "        train_loss_results = list()\n",
    "        val_loss_results = list()\n",
    "\n",
    "        train_accuracy_results = list()\n",
    "        val_accuracy_results = list()\n",
    "        \n",
    "        for e in range(epoch):\n",
    "            p = tf.keras.metrics.Mean()\n",
    "            epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "            epoch_loss_avg_val = tf.keras.metrics.Mean()\n",
    "\n",
    "            epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "            epoch_accuracy_val = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "            for x, y in train_seq:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    prediction = self._forward(x)\n",
    "                    loss_value = loss(y, prediction)\n",
    "                    # epoch_train_loss.append(loss_value.mean())\n",
    "                gradients = tape.gradient(loss_value, self._trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, self._trainable_variables))\n",
    "                epoch_accuracy.update_state(y, prediction)\n",
    "                epoch_loss_avg.update_state(loss_value)\n",
    "                \n",
    "            train_accuracy_results.append(epoch_accuracy.result().numpy())\n",
    "            train_loss_results.append(epoch_loss_avg.result().numpy())\n",
    "\n",
    "\n",
    "            for x, y in eval_seq:\n",
    "                prediction = self._forward(x)\n",
    "                loss_value = loss(y, prediction)\n",
    "                epoch_loss_avg_val.update_state(loss_value)\n",
    "                epoch_accuracy_val.update_state(y, prediction)\n",
    "            \n",
    "            val_accuracy_results.append(epoch_accuracy_val.result().numpy())\n",
    "            val_loss_results.append(epoch_loss_avg_val.result().numpy())\n",
    "\n",
    "            # print(f\"Epoch train loss: {epoch_train_loss[-1]:.2f},\\nEpoch val loss: {epoch_val_loss[-1]:.2f}\\n{'-'*20}\")\n",
    "            print(\"Epoch {}: Train loss: {:.3f} Train Accuracy: {:.3f}\".format(e + 1,\n",
    "                                                                               train_loss_results[-1],\n",
    "                                                                               train_accuracy_results[-1]))\n",
    "            print(\"Epoch {}: Val loss: {:.3f} Val Accuracy: {:.3f}\".format(e + 1,\n",
    "                                                                           val_loss_results[-1],\n",
    "                                                                           val_accuracy_results[-1]))\n",
    "            print('*' * 20)\n",
    "\n",
    "#             history['train'].append(epoch_train_loss)\n",
    "#             history['val'].append(epoch_val_loss)\n",
    "\n",
    "        return history\n",
    "            \n",
    "    def predict_generator(self, seq):\n",
    "        predictions = list()\n",
    "        for x in seq:\n",
    "            predictions.append(self._forward(x).numpy())\n",
    "        return np.vstack(predictions)\n",
    "    \n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return self._trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 0.343 Train Accuracy: 0.898\n",
      "Epoch 1: Val loss: 0.193 Val Accuracy: 0.941\n",
      "********************\n",
      "Epoch 2: Train loss: 0.138 Train Accuracy: 0.959\n",
      "Epoch 2: Val loss: 0.150 Val Accuracy: 0.953\n",
      "********************\n",
      "Epoch 3: Train loss: 0.094 Train Accuracy: 0.972\n",
      "Epoch 3: Val loss: 0.124 Val Accuracy: 0.963\n",
      "********************\n",
      "Epoch 4: Train loss: 0.066 Train Accuracy: 0.980\n",
      "Epoch 4: Val loss: 0.125 Val Accuracy: 0.963\n",
      "********************\n",
      "Epoch 5: Train loss: 0.050 Train Accuracy: 0.985\n",
      "Epoch 5: Val loss: 0.120 Val Accuracy: 0.965\n",
      "********************\n",
      "Epoch 6: Train loss: 0.039 Train Accuracy: 0.988\n",
      "Epoch 6: Val loss: 0.118 Val Accuracy: 0.969\n",
      "********************\n",
      "Epoch 7: Train loss: 0.033 Train Accuracy: 0.989\n",
      "Epoch 7: Val loss: 0.138 Val Accuracy: 0.965\n",
      "********************\n",
      "Epoch 8: Train loss: 0.029 Train Accuracy: 0.990\n",
      "Epoch 8: Val loss: 0.130 Val Accuracy: 0.967\n",
      "********************\n",
      "Epoch 9: Train loss: 0.028 Train Accuracy: 0.990\n",
      "Epoch 9: Val loss: 0.126 Val Accuracy: 0.969\n",
      "********************\n",
      "Epoch 10: Train loss: 0.028 Train Accuracy: 0.991\n",
      "Epoch 10: Val loss: 0.159 Val Accuracy: 0.964\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Dense(784, 100, tf.nn.relu), \n",
    "                   Dense(100, 100, tf.nn.relu), \n",
    "                   Dense(100, 10, tf.nn.softmax))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                    keras.losses.sparse_categorical_crossentropy, \n",
    "                    keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 4.693 Train Accuracy: 0.596\n",
      "Epoch 1: Val loss: 0.360 Val Accuracy: 0.894\n",
      "********************\n",
      "Epoch 2: Train loss: 0.273 Train Accuracy: 0.919\n",
      "Epoch 2: Val loss: 0.217 Val Accuracy: 0.935\n",
      "********************\n",
      "Epoch 3: Train loss: 0.191 Train Accuracy: 0.944\n",
      "Epoch 3: Val loss: 0.170 Val Accuracy: 0.950\n",
      "********************\n",
      "Epoch 4: Train loss: 0.149 Train Accuracy: 0.956\n",
      "Epoch 4: Val loss: 0.143 Val Accuracy: 0.958\n",
      "********************\n",
      "Epoch 5: Train loss: 0.122 Train Accuracy: 0.964\n",
      "Epoch 5: Val loss: 0.125 Val Accuracy: 0.964\n",
      "********************\n",
      "Epoch 6: Train loss: 0.101 Train Accuracy: 0.970\n",
      "Epoch 6: Val loss: 0.114 Val Accuracy: 0.966\n",
      "********************\n",
      "Epoch 7: Train loss: 0.086 Train Accuracy: 0.974\n",
      "Epoch 7: Val loss: 0.107 Val Accuracy: 0.968\n",
      "********************\n",
      "Epoch 8: Train loss: 0.073 Train Accuracy: 0.978\n",
      "Epoch 8: Val loss: 0.102 Val Accuracy: 0.970\n",
      "********************\n",
      "Epoch 9: Train loss: 0.062 Train Accuracy: 0.982\n",
      "Epoch 9: Val loss: 0.099 Val Accuracy: 0.971\n",
      "********************\n",
      "Epoch 10: Train loss: 0.054 Train Accuracy: 0.985\n",
      "Epoch 10: Val loss: 0.097 Val Accuracy: 0.972\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Dense(784, 100, tf.nn.sigmoid), \n",
    "                   Dense(100, 100, tf.nn.sigmoid), \n",
    "                   Dense(100, 10, tf.nn.softmax))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                    keras.losses.sparse_categorical_crossentropy, \n",
    "                    keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_mod = MNISTSequence(X_tr, y_tr, 128, \n",
    "                              preprocess=lambda x, y: ((x.reshape((-1, 28 * 28)) / 255. - 0.5) * 2, y))\n",
    "test_seq_mod = MNISTSequence(X_test, y_test, 128,\n",
    "                             preprocess=lambda x, y: ((x.reshape((-1, 28 * 28)) / 255. - 0.5) * 2, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Создаем слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.layers.core.Dense object at 0x7f07f051bb00>,\n",
      " <tensorflow.python.keras.layers.core.Dense object at 0x7f07f0585588>,\n",
      " <tensorflow.python.keras.layers.core.Dense object at 0x7f07f051b828>]\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "layers = [tf.keras.layers.Dense(hidden_size, activation=tf.nn.relu) for _ in range(2)]\n",
    "layers.append(keras.layers.Dense(10, activation='softmax'))  # <-- Обратите внимание! Можно передавать имена объектов\n",
    "pprint(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Компилируем сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = tf.keras.Sequential(layers)\n",
    "perceptron.compile(keras.optimizers.Adam(),\n",
    "                   loss=keras.losses.sparse_categorical_crossentropy,\n",
    "                   metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Обучаем сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-98a79d90bb01>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 469 steps, validate for 79 steps\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3507 - sparse_categorical_accuracy: 0.9002 - val_loss: 0.1897 - val_sparse_categorical_accuracy: 0.9433\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1473 - sparse_categorical_accuracy: 0.9568 - val_loss: 0.1189 - val_sparse_categorical_accuracy: 0.9640\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1041 - sparse_categorical_accuracy: 0.9690 - val_loss: 0.0981 - val_sparse_categorical_accuracy: 0.9700\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0833 - sparse_categorical_accuracy: 0.9748 - val_loss: 0.0901 - val_sparse_categorical_accuracy: 0.9718\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0661 - sparse_categorical_accuracy: 0.9797 - val_loss: 0.0837 - val_sparse_categorical_accuracy: 0.9731\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0529 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.0935 - val_sparse_categorical_accuracy: 0.9711\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0435 - sparse_categorical_accuracy: 0.9868 - val_loss: 0.0768 - val_sparse_categorical_accuracy: 0.9765\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9885 - val_loss: 0.0833 - val_sparse_categorical_accuracy: 0.9768\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0322 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.0815 - val_sparse_categorical_accuracy: 0.9759\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0286 - sparse_categorical_accuracy: 0.9914 - val_loss: 0.0852 - val_sparse_categorical_accuracy: 0.9745\n"
     ]
    }
   ],
   "source": [
    "history = perceptron.fit_generator(train_seq, validation_data=test_seq, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Все одним блоком\n",
    "\n",
    "* Кроме подготовки данных. \n",
    "* Обратите внимание: активации, оптимизатор, метрика ... заданы строками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 469 steps\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3667 - sparse_categorical_accuracy: 0.8953\n",
      "Epoch 2/10\n",
      "368/469 [======================>.......] - ETA: 0s - loss: 0.1478 - sparse_categorical_accuracy: 0.9563"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-759117325e08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m history = perceptron.fit_generator(train_seq, \n\u001b[1;32m     10\u001b[0m                                    \u001b[0;31m# validation_data=test_seq,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                    epochs=10)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0mcurrent_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     with training_context.on_batch(\n\u001b[0;32m--> 126\u001b[0;31m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0m\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[0;34m(self, step, mode, size)\u001b[0m\n\u001b[1;32m    766\u001b[0m       \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m       self.callbacks._call_batch_hook(\n\u001b[0;32m--> 768\u001b[0;31m           mode, 'begin', step, batch_logs)\n\u001b[0m\u001b[1;32m    769\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m       \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m       \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \"\"\"\n\u001b[1;32m    514\u001b[0m     \u001b[0;31m# For backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_subclass_implementers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "layers = [tf.keras.layers.Dense(hidden_size, activation='relu') for _ in range(2)]\n",
    "layers.append(keras.layers.Dense(10, activation='softmax'))  # <-- Обратите внимание! Можно передавать имена объектов\n",
    "\n",
    "perceptron = tf.keras.Sequential(layers)\n",
    "perceptron.compile('adam',\n",
    "                   'sparse_categorical_crossentropy',\n",
    "                   metrics=['sparse_categorical_accuracy'])\n",
    "history = perceptron.fit_generator(train_seq, \n",
    "                                   validation_data=test_seq,\n",
    "                                   epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "> Сначала создадим директорию для записи логов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxrwxr-x 7 1009 1009   4096 Jul 31 18:38 AAE\r\n",
      "drwxrwxr-x 2 1009 1009   4096 Jul  7 20:14 Container\r\n",
      "drwxrwxr-x 5 1009 1009   4096 Jul  3 19:13 conv_nets\r\n",
      "drwxrwxr-x 7 1009 1009   4096 Jun 27 10:05 gradients\r\n",
      "drwxrwxr-x 3 1009 1009   4096 Jul 17 10:52 homework\r\n",
      "drwxrwxr-x 2 1009 1009   4096 Jun 19 08:54 img\r\n",
      "drwxrwxr-x 2 1009 1009  12288 Dec 18 14:22 logs\r\n",
      "drwxrwxr-x 5 1009 1009   4096 Dec 14 09:03 regularization\r\n",
      "drwxrwxr-x 6 1009 1009   4096 Jul 13 11:31 tensorflow_lstm\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p logs\n",
    "!ls -l | grep \"^d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(log_dir='logs',\n",
    "                                             histogram_freq=1,\n",
    "                                             write_graph=True,\n",
    "                                             write_grads=True,\n",
    "                                             write_images=True,\n",
    "                                             update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 469 steps, validate for 79 steps\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3534 - sparse_categorical_accuracy: 0.8991 - val_loss: 0.1837 - val_sparse_categorical_accuracy: 0.9449\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1520 - sparse_categorical_accuracy: 0.9555 - val_loss: 0.1295 - val_sparse_categorical_accuracy: 0.9611\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1060 - sparse_categorical_accuracy: 0.9683 - val_loss: 0.1087 - val_sparse_categorical_accuracy: 0.9657\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0811 - sparse_categorical_accuracy: 0.9749 - val_loss: 0.0881 - val_sparse_categorical_accuracy: 0.9726\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0666 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.0904 - val_sparse_categorical_accuracy: 0.9712\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0531 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.0799 - val_sparse_categorical_accuracy: 0.9749\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0436 - sparse_categorical_accuracy: 0.9862 - val_loss: 0.0782 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0361 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0741 - val_sparse_categorical_accuracy: 0.9774\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0311 - sparse_categorical_accuracy: 0.9903 - val_loss: 0.0721 - val_sparse_categorical_accuracy: 0.9783\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0260 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.0793 - val_sparse_categorical_accuracy: 0.9762\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "layers = [tf.keras.layers.Dense(hidden_size, activation='relu') for _ in range(2)]\n",
    "layers.append(keras.layers.Dense(10, activation='softmax'))  # <-- Обратите внимание! Можно передавать имена объектов\n",
    "\n",
    "perceptron = tf.keras.Sequential(layers)\n",
    "perceptron.compile('adam',\n",
    "                   'sparse_categorical_crossentropy',\n",
    "                   metrics=['sparse_categorical_accuracy']\n",
    "                  )\n",
    "history = perceptron.fit_generator(train_seq, \n",
    "                                   validation_data=test_seq,\n",
    "                                   epochs=10, \n",
    "                                   callbacks=[tensorboard_cb]\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 469 steps, validate for 79 steps\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2798 - sparse_categorical_accuracy: 0.9186 - val_loss: 0.1362 - val_sparse_categorical_accuracy: 0.9559\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1091 - sparse_categorical_accuracy: 0.9673 - val_loss: 0.1140 - val_sparse_categorical_accuracy: 0.9648\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0732 - sparse_categorical_accuracy: 0.9783 - val_loss: 0.0922 - val_sparse_categorical_accuracy: 0.9706\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0531 - sparse_categorical_accuracy: 0.9839 - val_loss: 0.0855 - val_sparse_categorical_accuracy: 0.9729\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0399 - sparse_categorical_accuracy: 0.9876 - val_loss: 0.0857 - val_sparse_categorical_accuracy: 0.9741\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0311 - sparse_categorical_accuracy: 0.9903 - val_loss: 0.0882 - val_sparse_categorical_accuracy: 0.9740\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0232 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.0941 - val_sparse_categorical_accuracy: 0.9747\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9943 - val_loss: 0.1053 - val_sparse_categorical_accuracy: 0.9725\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0170 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.0935 - val_sparse_categorical_accuracy: 0.9762\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0130 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.1004 - val_sparse_categorical_accuracy: 0.9752\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(log_dir='logs',\n",
    "                                             histogram_freq=1,\n",
    "                                             write_graph=True,\n",
    "                                             write_grads=True,\n",
    "                                             write_images=True,\n",
    "                                             update_freq='batch')\n",
    "\n",
    "hidden_size = 100\n",
    "layers = [\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "]\n",
    "\n",
    "perceptron = tf.keras.Sequential(layers)\n",
    "perceptron.compile('adam',\n",
    "                   'sparse_categorical_crossentropy',\n",
    "                    metrics=['sparse_categorical_accuracy']\n",
    "                  )\n",
    "history = perceptron.fit_generator(train_seq, \n",
    "                                   validation_data=test_seq,\n",
    "                                   epochs=10, \n",
    "                                   callbacks=[tensorboard_cb]\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Но не все так гладко: сложно получить доступ к промежуточным значениям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://github.com/tensorflow/tensorflow/issues/33478"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Низкоуровневый tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, inp_shape, out_shape, activation: Callable, name):\n",
    "        self.trainable = True\n",
    "        self._inp_shape = inp_shape\n",
    "        self._out_shape = out_shape\n",
    "        self._activation = activation\n",
    "        if 'sigmoid' in self._activation.__name__:\n",
    "            self._w = tf.Variable(np.random.rand(inp_shape, out_shape) * np.sqrt(6 / (inp_shape + out_shape)))\n",
    "        elif 'relu' in self._activation.__name__:\n",
    "            self._w = tf.Variable(np.random.randn(inp_shape, out_shape) * np.sqrt(2 / (inp_shape)))\n",
    "        else:\n",
    "            # Just a Normal\n",
    "            self._w = tf.Variable(np.random.randn(inp_shape, out_shape))\n",
    "        self._b = tf.Variable(np.zeros((1, out_shape)))\n",
    "        self.name = name\n",
    "        \n",
    "    def __call__(self, x, writer=None, step=None):\n",
    "        val = x @ self._w + self._b\n",
    "        a = self._activation(val)\n",
    "        if writer is not None:\n",
    "            with writer.as_default():\n",
    "                tf.summary.histogram(self.name + '_kernel', self._w, step=step)\n",
    "                tf.summary.histogram(self.name + '_bias', self._b, step=step)\n",
    "                tf.summary.histogram(self.name + '_activation', a, step=step)\n",
    "                tf.summary.histogram(self.name + '_z', val, step=step)\n",
    "        return a\n",
    "    \n",
    "    def get_trainable(self):\n",
    "        if self.trainable: \n",
    "            return [self._w, self._b]\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    @property\n",
    "    def inp_shape(self):\n",
    "        return self._inp_shape\n",
    "    \n",
    "    @property\n",
    "    def out_shape(self):\n",
    "        return self._out_shape\n",
    "    \n",
    "    @property\n",
    "    def w(self):\n",
    "        return self._w\n",
    "    \n",
    "    @property\n",
    "    def b(self):\n",
    "        return self._b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, *args):\n",
    "        self._layers = args\n",
    "        self._trainable_variables = [i for s in [l.get_trainable() for l in self._layers] for i in s] \n",
    "        \n",
    "    def _forward(self, x, writer=None, step=None):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x, writer, step)\n",
    "        return x\n",
    "        \n",
    "    def fit_generator(self, train_seq, eval_seq, epoch, loss, optimizer, writer=None):\n",
    "        history = dict(train=list(), val=list())\n",
    "        \n",
    "        train_loss_results = list()\n",
    "        val_loss_results = list()\n",
    "\n",
    "        train_accuracy_results = list()\n",
    "        val_accuracy_results = list()\n",
    "        \n",
    "        step = 0\n",
    "        for e in range(epoch):\n",
    "            p = tf.keras.metrics.Mean()\n",
    "            epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "            epoch_loss_avg_val = tf.keras.metrics.Mean()\n",
    "\n",
    "            epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "            epoch_accuracy_val = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "            for x, y in train_seq:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    prediction = self._forward(x, writer, step)\n",
    "                    loss_value = loss(y, prediction)\n",
    "                    # epoch_train_loss.append(loss_value.mean())\n",
    "                gradients = tape.gradient(loss_value, self._trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, self._trainable_variables))\n",
    "                epoch_accuracy.update_state(y, prediction)\n",
    "                epoch_loss_avg.update_state(loss_value)\n",
    "                with writer.as_default():\n",
    "                    tf.summary.scalar('train_accuracy', epoch_accuracy.result().numpy(), step=step)\n",
    "                    tf.summary.scalar('train_loss', epoch_loss_avg.result().numpy(), step=step)\n",
    "\n",
    "                step += 1\n",
    "                \n",
    "            train_accuracy_results.append(epoch_accuracy.result().numpy())\n",
    "            train_loss_results.append(epoch_loss_avg.result().numpy())\n",
    "\n",
    "\n",
    "            for x, y in eval_seq:\n",
    "                prediction = self._forward(x)\n",
    "                loss_value = loss(y, prediction)\n",
    "                epoch_loss_avg_val.update_state(loss_value)\n",
    "                epoch_accuracy_val.update_state(y, prediction)\n",
    "            \n",
    "            val_accuracy_results.append(epoch_accuracy_val.result().numpy())\n",
    "            val_loss_results.append(epoch_loss_avg_val.result().numpy())\n",
    "\n",
    "            # print(f\"Epoch train loss: {epoch_train_loss[-1]:.2f},\\nEpoch val loss: {epoch_val_loss[-1]:.2f}\\n{'-'*20}\")\n",
    "            print(\"Epoch {}: Train loss: {:.3f} Train Accuracy: {:.3f}\".format(e + 1,\n",
    "                                                                               train_loss_results[-1],\n",
    "                                                                               train_accuracy_results[-1]))\n",
    "            print(\"Epoch {}: Val loss: {:.3f} Val Accuracy: {:.3f}\".format(e + 1,\n",
    "                                                                           val_loss_results[-1],\n",
    "                                                                           val_accuracy_results[-1]))\n",
    "            print('*' * 20)\n",
    "\n",
    "        return None\n",
    "            \n",
    "    def predict_generator(self, seq):\n",
    "        predictions = list()\n",
    "        for x in seq:\n",
    "            predictions.append(self._forward(x).numpy())\n",
    "        return np.vstack(predictions)\n",
    "    \n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return self._trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 0.330 Train Accuracy: 0.903\n",
      "Epoch 1: Val loss: 0.175 Val Accuracy: 0.946\n",
      "********************\n",
      "Epoch 2: Train loss: 0.134 Train Accuracy: 0.960\n",
      "Epoch 2: Val loss: 0.149 Val Accuracy: 0.956\n",
      "********************\n",
      "Epoch 3: Train loss: 0.091 Train Accuracy: 0.972\n",
      "Epoch 3: Val loss: 0.136 Val Accuracy: 0.961\n",
      "********************\n",
      "Epoch 4: Train loss: 0.068 Train Accuracy: 0.979\n",
      "Epoch 4: Val loss: 0.126 Val Accuracy: 0.964\n",
      "********************\n",
      "Epoch 5: Train loss: 0.052 Train Accuracy: 0.984\n",
      "Epoch 5: Val loss: 0.125 Val Accuracy: 0.965\n",
      "********************\n",
      "Epoch 6: Train loss: 0.039 Train Accuracy: 0.988\n",
      "Epoch 6: Val loss: 0.123 Val Accuracy: 0.966\n",
      "********************\n",
      "Epoch 7: Train loss: 0.032 Train Accuracy: 0.990\n",
      "Epoch 7: Val loss: 0.136 Val Accuracy: 0.966\n",
      "********************\n",
      "Epoch 8: Train loss: 0.028 Train Accuracy: 0.991\n",
      "Epoch 8: Val loss: 0.139 Val Accuracy: 0.965\n",
      "********************\n",
      "Epoch 9: Train loss: 0.024 Train Accuracy: 0.992\n",
      "Epoch 9: Val loss: 0.126 Val Accuracy: 0.970\n",
      "********************\n",
      "Epoch 10: Train loss: 0.022 Train Accuracy: 0.993\n",
      "Epoch 10: Val loss: 0.145 Val Accuracy: 0.966\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Dense(784, 100, tf.nn.relu, 'dense'), \n",
    "                   Dense(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                            keras.losses.sparse_categorical_crossentropy, \n",
    "                            keras.optimizers.Adam(),\n",
    "                           writer\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 2.726 Train Accuracy: 0.671\n",
      "Epoch 1: Val loss: 1.849 Val Accuracy: 0.836\n",
      "********************\n",
      "Epoch 2: Train loss: 1.839 Train Accuracy: 0.842\n",
      "Epoch 2: Val loss: 1.790 Val Accuracy: 0.851\n",
      "********************\n",
      "Epoch 3: Train loss: 1.792 Train Accuracy: 0.855\n",
      "Epoch 3: Val loss: 1.760 Val Accuracy: 0.859\n",
      "********************\n",
      "Epoch 4: Train loss: 0.970 Train Accuracy: 0.900\n",
      "Epoch 4: Val loss: 0.156 Val Accuracy: 0.952\n",
      "********************\n",
      "Epoch 5: Train loss: 0.137 Train Accuracy: 0.960\n",
      "Epoch 5: Val loss: 0.127 Val Accuracy: 0.963\n",
      "********************\n",
      "Epoch 6: Train loss: 0.111 Train Accuracy: 0.967\n",
      "Epoch 6: Val loss: 0.112 Val Accuracy: 0.967\n",
      "********************\n",
      "Epoch 7: Train loss: 0.093 Train Accuracy: 0.972\n",
      "Epoch 7: Val loss: 0.104 Val Accuracy: 0.970\n",
      "********************\n",
      "Epoch 8: Train loss: 0.080 Train Accuracy: 0.977\n",
      "Epoch 8: Val loss: 0.099 Val Accuracy: 0.972\n",
      "********************\n",
      "Epoch 9: Train loss: 0.068 Train Accuracy: 0.980\n",
      "Epoch 9: Val loss: 0.095 Val Accuracy: 0.973\n",
      "********************\n",
      "Epoch 10: Train loss: 0.059 Train Accuracy: 0.983\n",
      "Epoch 10: Val loss: 0.093 Val Accuracy: 0.972\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Dense(784, 100, tf.nn.sigmoid, 'dense'), \n",
    "                   Dense(100, 100, tf.nn.sigmoid, 'dense1'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                            keras.losses.sparse_categorical_crossentropy, \n",
    "                            keras.optimizers.Adam(),\n",
    "                           writer\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class BatchNorm:\n",
    "    def __init__(self, name):\n",
    "        self.trainable = True\n",
    "        self._beta = tf.Variable(0, dtype='float64')\n",
    "        self._gamma = tf.Variable(1,  dtype='float64')\n",
    "        self.name = name\n",
    "        \n",
    "    def __call__(self, x, writer=None, step=None):\n",
    "        mu = tf.reduce_mean(x, axis=0)\n",
    "        sigma = tf.math.reduce_std(x, axis=0)\n",
    "        normed = (x - mu) / sigma # !!\n",
    "        out = normed * self._gamma + self._beta\n",
    "        \n",
    "        if writer is not None:\n",
    "            with writer.as_default():\n",
    "                tf.summary.histogram(self.name + '_beta', self._beta, step=step)\n",
    "                tf.summary.histogram(self.name + '_gamma', self._gamma, step=step)\n",
    "                tf.summary.histogram(self.name + '_normed', normed, step=step)\n",
    "                tf.summary.histogram(self.name + '_out', out, step=step)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_trainable(self):\n",
    "        if self.trainable: \n",
    "            return [self._beta, self._gamma]\n",
    "        else:\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/batch_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 1.925 Train Accuracy: 0.742\n",
      "Epoch 1: Val loss: 0.269 Val Accuracy: 0.926\n",
      "********************\n",
      "Epoch 2: Train loss: 0.218 Train Accuracy: 0.939\n",
      "Epoch 2: Val loss: 0.183 Val Accuracy: 0.948\n",
      "********************\n",
      "Epoch 3: Train loss: 0.145 Train Accuracy: 0.958\n",
      "Epoch 3: Val loss: 0.146 Val Accuracy: 0.958\n",
      "********************\n",
      "Epoch 4: Train loss: 0.106 Train Accuracy: 0.969\n",
      "Epoch 4: Val loss: 0.128 Val Accuracy: 0.962\n",
      "********************\n",
      "Epoch 5: Train loss: 0.082 Train Accuracy: 0.976\n",
      "Epoch 5: Val loss: 0.122 Val Accuracy: 0.964\n",
      "********************\n",
      "Epoch 6: Train loss: 0.064 Train Accuracy: 0.981\n",
      "Epoch 6: Val loss: 0.119 Val Accuracy: 0.966\n",
      "********************\n",
      "Epoch 7: Train loss: 0.049 Train Accuracy: 0.986\n",
      "Epoch 7: Val loss: 0.118 Val Accuracy: 0.967\n",
      "********************\n",
      "Epoch 8: Train loss: 0.037 Train Accuracy: 0.990\n",
      "Epoch 8: Val loss: 0.118 Val Accuracy: 0.968\n",
      "********************\n",
      "Epoch 9: Train loss: 0.028 Train Accuracy: 0.993\n",
      "Epoch 9: Val loss: 0.120 Val Accuracy: 0.968\n",
      "********************\n",
      "Epoch 10: Train loss: 0.020 Train Accuracy: 0.996\n",
      "Epoch 10: Val loss: 0.123 Val Accuracy: 0.969\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Dense(784, 100, tf.nn.sigmoid, 'dense'), \n",
    "                   BatchNorm('batch_norm'), \n",
    "                   Dense(100, 100, tf.nn.sigmoid, 'dense1'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                            keras.losses.sparse_categorical_crossentropy, \n",
    "                            keras.optimizers.Adam(),\n",
    "                           writer\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
